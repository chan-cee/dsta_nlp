{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nlp_title</th>\n",
       "      <th>nlp_text</th>\n",
       "      <th>nlp_comments</th>\n",
       "      <th>nlp_class</th>\n",
       "      <th>cv_filename</th>\n",
       "      <th>cv_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>attend a gp like that and see if you can sneak...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>i dont understand your halloween costume eithe...</td>\n",
       "      <td>real</td>\n",
       "      <td>0.jpg</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>misleading title.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>blaming the president for a software deficit. ...</td>\n",
       "      <td>fake</td>\n",
       "      <td>1.jpg</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nah, thats still vegan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>it just means it made in the same factory as o...</td>\n",
       "      <td>fake</td>\n",
       "      <td>2.jpg</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>confusing article. it starts out talk about do...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>this entire article is basically the ceo sayin...</td>\n",
       "      <td>fake</td>\n",
       "      <td>3.jpg</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ag barr just resigned as well.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>i hope that biden does well for the same reaso...</td>\n",
       "      <td>real</td>\n",
       "      <td>4.jpg</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7995</th>\n",
       "      <td>fresh herbs close to kitchen.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>officer: whats that smell? atleast use the sam...</td>\n",
       "      <td>real</td>\n",
       "      <td>7995.jpg</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7996</th>\n",
       "      <td>years back in ca a driver would use the carpoo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>take it to court! original dallas morning news...</td>\n",
       "      <td>real</td>\n",
       "      <td>7996.jpg</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7997</th>\n",
       "      <td>it's only bitcoin, you can always replace it!</td>\n",
       "      <td>NaN</td>\n",
       "      <td>this is one time i approve of not holding. all...</td>\n",
       "      <td>real</td>\n",
       "      <td>7997.jpg</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7998</th>\n",
       "      <td>this is my hometown. i had to deal with these ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>one girl saw the harry potter book that jessic...</td>\n",
       "      <td>real</td>\n",
       "      <td>7998.jpg</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7999</th>\n",
       "      <td>some don't like scientists talking re #climate...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fake</td>\n",
       "      <td>7999.jpg</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              nlp_title nlp_text  \\\n",
       "0     attend a gp like that and see if you can sneak...      NaN   \n",
       "1                                     misleading title.      NaN   \n",
       "2                                nah, thats still vegan      NaN   \n",
       "3     confusing article. it starts out talk about do...      NaN   \n",
       "4                        ag barr just resigned as well.      NaN   \n",
       "...                                                 ...      ...   \n",
       "7995                      fresh herbs close to kitchen.      NaN   \n",
       "7996  years back in ca a driver would use the carpoo...      NaN   \n",
       "7997      it's only bitcoin, you can always replace it!      NaN   \n",
       "7998  this is my hometown. i had to deal with these ...      NaN   \n",
       "7999  some don't like scientists talking re #climate...      NaN   \n",
       "\n",
       "                                           nlp_comments nlp_class cv_filename  \\\n",
       "0     i dont understand your halloween costume eithe...      real       0.jpg   \n",
       "1     blaming the president for a software deficit. ...      fake       1.jpg   \n",
       "2     it just means it made in the same factory as o...      fake       2.jpg   \n",
       "3     this entire article is basically the ceo sayin...      fake       3.jpg   \n",
       "4     i hope that biden does well for the same reaso...      real       4.jpg   \n",
       "...                                                 ...       ...         ...   \n",
       "7995  officer: whats that smell? atleast use the sam...      real    7995.jpg   \n",
       "7996  take it to court! original dallas morning news...      real    7996.jpg   \n",
       "7997  this is one time i approve of not holding. all...      real    7997.jpg   \n",
       "7998  one girl saw the harry potter book that jessic...      real    7998.jpg   \n",
       "7999                                                NaN      fake    7999.jpg   \n",
       "\n",
       "     cv_class  \n",
       "0        real  \n",
       "1        real  \n",
       "2        fake  \n",
       "3        real  \n",
       "4        real  \n",
       "...       ...  \n",
       "7995     real  \n",
       "7996     real  \n",
       "7997     fake  \n",
       "7998     real  \n",
       "7999     fake  \n",
       "\n",
       "[8000 rows x 6 columns]"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "614\n",
      "1       False\n",
      "2       False\n",
      "3       False\n",
      "6       False\n",
      "7       False\n",
      "        ...  \n",
      "7990    False\n",
      "7991    False\n",
      "7992    False\n",
      "7993    False\n",
      "7999     True\n",
      "Name: nlp_title, Length: 3424, dtype: bool\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv(\"train.csv\")\n",
    "result = test[test['nlp_class'] == 'fake']['nlp_title'].str.contains(\"https\")\n",
    "true_count = (result == True).sum()\n",
    "print(true_count)\n",
    "print(result)\n",
    "\n",
    "result2 = test[test['nlp_class'] == 'real']['nlp_title'].str.contains(\"https\")\n",
    "true_count2 = (result2 == True).sum()\n",
    "print(true_count2)\n",
    "\n",
    "na = test[test['nlp_text'].na]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nlp_title</th>\n",
       "      <th>nlp_text</th>\n",
       "      <th>nlp_comments</th>\n",
       "      <th>nlp_class</th>\n",
       "      <th>cv_filename</th>\n",
       "      <th>cv_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>@senwarren @realdonaldtrump you can't talk abo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fake</td>\n",
       "      <td>161.jpg</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>woman testifies that #jeffreyepstein flew her ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fake</td>\n",
       "      <td>234.jpg</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>609</th>\n",
       "      <td>remember when trump got impeached for a phone ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>naturally, hes one of them. the fbi as an inst...</td>\n",
       "      <td>fake</td>\n",
       "      <td>609.jpg</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>669</th>\n",
       "      <td>rt @lacadri34: can someone tell trump he can't...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fake</td>\n",
       "      <td>669.jpg</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>748</th>\n",
       "      <td>how the oil industry created a ?deep state? in...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fake</td>\n",
       "      <td>748.jpg</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7418</th>\n",
       "      <td>false title. trump was backstage and then bega...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sorry i am a bit out of the loop on this can s...</td>\n",
       "      <td>fake</td>\n",
       "      <td>7418.jpg</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7511</th>\n",
       "      <td>um, a bunch of the narrative and quotes attrib...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>god i hate when fake articles get thousands of...</td>\n",
       "      <td>fake</td>\n",
       "      <td>7511.jpg</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7793</th>\n",
       "      <td>perfect spot for my little copypasta: this is ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>oh***and the plot thickens. im going q conspir...</td>\n",
       "      <td>fake</td>\n",
       "      <td>7793.jpg</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7921</th>\n",
       "      <td>@susamorgan @oufenix @orangethrowup @drylyrile...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fake</td>\n",
       "      <td>7921.jpg</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7934</th>\n",
       "      <td>what has populism given us really? division, h...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fake</td>\n",
       "      <td>7934.jpg</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>93 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              nlp_title nlp_text  \\\n",
       "161   @senwarren @realdonaldtrump you can't talk abo...      NaN   \n",
       "234   woman testifies that #jeffreyepstein flew her ...      NaN   \n",
       "609   remember when trump got impeached for a phone ...      NaN   \n",
       "669   rt @lacadri34: can someone tell trump he can't...      NaN   \n",
       "748   how the oil industry created a ?deep state? in...      NaN   \n",
       "...                                                 ...      ...   \n",
       "7418  false title. trump was backstage and then bega...      NaN   \n",
       "7511  um, a bunch of the narrative and quotes attrib...      NaN   \n",
       "7793  perfect spot for my little copypasta: this is ...      NaN   \n",
       "7921  @susamorgan @oufenix @orangethrowup @drylyrile...      NaN   \n",
       "7934  what has populism given us really? division, h...      NaN   \n",
       "\n",
       "                                           nlp_comments nlp_class cv_filename  \\\n",
       "161                                                 NaN      fake     161.jpg   \n",
       "234                                                 NaN      fake     234.jpg   \n",
       "609   naturally, hes one of them. the fbi as an inst...      fake     609.jpg   \n",
       "669                                                 NaN      fake     669.jpg   \n",
       "748                                                 NaN      fake     748.jpg   \n",
       "...                                                 ...       ...         ...   \n",
       "7418  sorry i am a bit out of the loop on this can s...      fake    7418.jpg   \n",
       "7511  god i hate when fake articles get thousands of...      fake    7511.jpg   \n",
       "7793  oh***and the plot thickens. im going q conspir...      fake    7793.jpg   \n",
       "7921                                                NaN      fake    7921.jpg   \n",
       "7934                                                NaN      fake    7934.jpg   \n",
       "\n",
       "     cv_class  \n",
       "161      real  \n",
       "234      real  \n",
       "609      fake  \n",
       "669      real  \n",
       "748      fake  \n",
       "...       ...  \n",
       "7418     fake  \n",
       "7511     real  \n",
       "7793     real  \n",
       "7921     real  \n",
       "7934     real  \n",
       "\n",
       "[93 rows x 6 columns]"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "na = test[test['nlp_title'].fillna('').str.contains(\"\")]\n",
    "na\n",
    "na[na[\"nlp_class\"] == \"fake\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nlp_title</th>\n",
       "      <th>nlp_text</th>\n",
       "      <th>nlp_comments</th>\n",
       "      <th>nlp_class</th>\n",
       "      <th>cv_filename</th>\n",
       "      <th>cv_class</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>attend a gp like that and see if you can sneak...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>i dont understand your halloween costume eithe...</td>\n",
       "      <td>real</td>\n",
       "      <td>0.jpg</td>\n",
       "      <td>real</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>misleading title.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>blaming the president for a software deficit. ...</td>\n",
       "      <td>fake</td>\n",
       "      <td>1.jpg</td>\n",
       "      <td>real</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nah, thats still vegan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>it just means it made in the same factory as o...</td>\n",
       "      <td>fake</td>\n",
       "      <td>2.jpg</td>\n",
       "      <td>fake</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>confusing article. it starts out talk about do...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>this entire article is basically the ceo sayin...</td>\n",
       "      <td>fake</td>\n",
       "      <td>3.jpg</td>\n",
       "      <td>real</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ag barr just resigned as well.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>i hope that biden does well for the same reaso...</td>\n",
       "      <td>real</td>\n",
       "      <td>4.jpg</td>\n",
       "      <td>real</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7995</th>\n",
       "      <td>fresh herbs close to kitchen.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>officer: whats that smell? atleast use the sam...</td>\n",
       "      <td>real</td>\n",
       "      <td>7995.jpg</td>\n",
       "      <td>real</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7996</th>\n",
       "      <td>years back in ca a driver would use the carpoo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>take it to court! original dallas morning news...</td>\n",
       "      <td>real</td>\n",
       "      <td>7996.jpg</td>\n",
       "      <td>real</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7997</th>\n",
       "      <td>it's only bitcoin, you can always replace it!</td>\n",
       "      <td>NaN</td>\n",
       "      <td>this is one time i approve of not holding. all...</td>\n",
       "      <td>real</td>\n",
       "      <td>7997.jpg</td>\n",
       "      <td>fake</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7998</th>\n",
       "      <td>this is my hometown. i had to deal with these ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>one girl saw the harry potter book that jessic...</td>\n",
       "      <td>real</td>\n",
       "      <td>7998.jpg</td>\n",
       "      <td>real</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7999</th>\n",
       "      <td>some don't like scientists talking re #climate...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fake</td>\n",
       "      <td>7999.jpg</td>\n",
       "      <td>fake</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              nlp_title nlp_text  \\\n",
       "0     attend a gp like that and see if you can sneak...      NaN   \n",
       "1                                     misleading title.      NaN   \n",
       "2                                nah, thats still vegan      NaN   \n",
       "3     confusing article. it starts out talk about do...      NaN   \n",
       "4                        ag barr just resigned as well.      NaN   \n",
       "...                                                 ...      ...   \n",
       "7995                      fresh herbs close to kitchen.      NaN   \n",
       "7996  years back in ca a driver would use the carpoo...      NaN   \n",
       "7997      it's only bitcoin, you can always replace it!      NaN   \n",
       "7998  this is my hometown. i had to deal with these ...      NaN   \n",
       "7999  some don't like scientists talking re #climate...      NaN   \n",
       "\n",
       "                                           nlp_comments nlp_class cv_filename  \\\n",
       "0     i dont understand your halloween costume eithe...      real       0.jpg   \n",
       "1     blaming the president for a software deficit. ...      fake       1.jpg   \n",
       "2     it just means it made in the same factory as o...      fake       2.jpg   \n",
       "3     this entire article is basically the ceo sayin...      fake       3.jpg   \n",
       "4     i hope that biden does well for the same reaso...      real       4.jpg   \n",
       "...                                                 ...       ...         ...   \n",
       "7995  officer: whats that smell? atleast use the sam...      real    7995.jpg   \n",
       "7996  take it to court! original dallas morning news...      real    7996.jpg   \n",
       "7997  this is one time i approve of not holding. all...      real    7997.jpg   \n",
       "7998  one girl saw the harry potter book that jessic...      real    7998.jpg   \n",
       "7999                                                NaN      fake    7999.jpg   \n",
       "\n",
       "     cv_class  target  \n",
       "0        real       1  \n",
       "1        real       0  \n",
       "2        fake       0  \n",
       "3        real       0  \n",
       "4        real       1  \n",
       "...       ...     ...  \n",
       "7995     real       1  \n",
       "7996     real       1  \n",
       "7997     fake       1  \n",
       "7998     real       1  \n",
       "7999     fake       0  \n",
       "\n",
       "[8000 rows x 7 columns]"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['target'] = df['nlp_class'].apply(lambda x : 1 if x == 'real' else 0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import transformers\n",
    "import tqdm\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_tokenizer(tokenizer_name, docs):\n",
    "    features = []\n",
    "    for doc in tqdm.tqdm(docs, desc = 'converting documents to features'):\n",
    "        tokens = tokenizer_name.tokenize(doc)\n",
    "        ids = tokenizer_name.convert_tokens_to_ids(tokens)\n",
    "        features.append(ids)\n",
    "    return features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# combine all three fields into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nlp_title</th>\n",
       "      <th>nlp_text</th>\n",
       "      <th>nlp_comments</th>\n",
       "      <th>nlp_class</th>\n",
       "      <th>cv_filename</th>\n",
       "      <th>cv_class</th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>attend a gp like that and see if you can sneak...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>i dont understand your halloween costume eithe...</td>\n",
       "      <td>real</td>\n",
       "      <td>0.jpg</td>\n",
       "      <td>real</td>\n",
       "      <td>1</td>\n",
       "      <td>attend a gp like that and see if you can sneak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>misleading title.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>blaming the president for a software deficit. ...</td>\n",
       "      <td>fake</td>\n",
       "      <td>1.jpg</td>\n",
       "      <td>real</td>\n",
       "      <td>0</td>\n",
       "      <td>misleading title.  blaming the president for a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nah, thats still vegan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>it just means it made in the same factory as o...</td>\n",
       "      <td>fake</td>\n",
       "      <td>2.jpg</td>\n",
       "      <td>fake</td>\n",
       "      <td>0</td>\n",
       "      <td>nah, thats still vegan  it just means it made ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>confusing article. it starts out talk about do...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>this entire article is basically the ceo sayin...</td>\n",
       "      <td>fake</td>\n",
       "      <td>3.jpg</td>\n",
       "      <td>real</td>\n",
       "      <td>0</td>\n",
       "      <td>confusing article. it starts out talk about do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ag barr just resigned as well.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>i hope that biden does well for the same reaso...</td>\n",
       "      <td>real</td>\n",
       "      <td>4.jpg</td>\n",
       "      <td>real</td>\n",
       "      <td>1</td>\n",
       "      <td>ag barr just resigned as well.  i hope that bi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7995</th>\n",
       "      <td>fresh herbs close to kitchen.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>officer: whats that smell? atleast use the sam...</td>\n",
       "      <td>real</td>\n",
       "      <td>7995.jpg</td>\n",
       "      <td>real</td>\n",
       "      <td>1</td>\n",
       "      <td>fresh herbs close to kitchen.  officer: whats ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7996</th>\n",
       "      <td>years back in ca a driver would use the carpoo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>take it to court! original dallas morning news...</td>\n",
       "      <td>real</td>\n",
       "      <td>7996.jpg</td>\n",
       "      <td>real</td>\n",
       "      <td>1</td>\n",
       "      <td>years back in ca a driver would use the carpoo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7997</th>\n",
       "      <td>it's only bitcoin, you can always replace it!</td>\n",
       "      <td>NaN</td>\n",
       "      <td>this is one time i approve of not holding. all...</td>\n",
       "      <td>real</td>\n",
       "      <td>7997.jpg</td>\n",
       "      <td>fake</td>\n",
       "      <td>1</td>\n",
       "      <td>it's only bitcoin, you can always replace it! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7998</th>\n",
       "      <td>this is my hometown. i had to deal with these ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>one girl saw the harry potter book that jessic...</td>\n",
       "      <td>real</td>\n",
       "      <td>7998.jpg</td>\n",
       "      <td>real</td>\n",
       "      <td>1</td>\n",
       "      <td>this is my hometown. i had to deal with these ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7999</th>\n",
       "      <td>some don't like scientists talking re #climate...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fake</td>\n",
       "      <td>7999.jpg</td>\n",
       "      <td>fake</td>\n",
       "      <td>0</td>\n",
       "      <td>some don't like scientists talking re #climate...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8000 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              nlp_title nlp_text  \\\n",
       "0     attend a gp like that and see if you can sneak...      NaN   \n",
       "1                                     misleading title.      NaN   \n",
       "2                                nah, thats still vegan      NaN   \n",
       "3     confusing article. it starts out talk about do...      NaN   \n",
       "4                        ag barr just resigned as well.      NaN   \n",
       "...                                                 ...      ...   \n",
       "7995                      fresh herbs close to kitchen.      NaN   \n",
       "7996  years back in ca a driver would use the carpoo...      NaN   \n",
       "7997      it's only bitcoin, you can always replace it!      NaN   \n",
       "7998  this is my hometown. i had to deal with these ...      NaN   \n",
       "7999  some don't like scientists talking re #climate...      NaN   \n",
       "\n",
       "                                           nlp_comments nlp_class cv_filename  \\\n",
       "0     i dont understand your halloween costume eithe...      real       0.jpg   \n",
       "1     blaming the president for a software deficit. ...      fake       1.jpg   \n",
       "2     it just means it made in the same factory as o...      fake       2.jpg   \n",
       "3     this entire article is basically the ceo sayin...      fake       3.jpg   \n",
       "4     i hope that biden does well for the same reaso...      real       4.jpg   \n",
       "...                                                 ...       ...         ...   \n",
       "7995  officer: whats that smell? atleast use the sam...      real    7995.jpg   \n",
       "7996  take it to court! original dallas morning news...      real    7996.jpg   \n",
       "7997  this is one time i approve of not holding. all...      real    7997.jpg   \n",
       "7998  one girl saw the harry potter book that jessic...      real    7998.jpg   \n",
       "7999                                                NaN      fake    7999.jpg   \n",
       "\n",
       "     cv_class  target                                               text  \n",
       "0        real       1  attend a gp like that and see if you can sneak...  \n",
       "1        real       0  misleading title.  blaming the president for a...  \n",
       "2        fake       0  nah, thats still vegan  it just means it made ...  \n",
       "3        real       0  confusing article. it starts out talk about do...  \n",
       "4        real       1  ag barr just resigned as well.  i hope that bi...  \n",
       "...       ...     ...                                                ...  \n",
       "7995     real       1  fresh herbs close to kitchen.  officer: whats ...  \n",
       "7996     real       1  years back in ca a driver would use the carpoo...  \n",
       "7997     fake       1  it's only bitcoin, you can always replace it! ...  \n",
       "7998     real       1  this is my hometown. i had to deal with these ...  \n",
       "7999     fake       0  some don't like scientists talking re #climate...  \n",
       "\n",
       "[8000 rows x 8 columns]"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'] = df['nlp_title'] + \" \" + df['nlp_text'].fillna('') + \" \" + df['nlp_comments'].astype(str)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>attend a gp like that and see if you can sneak...</td>\n",
       "      <td>i dont understand your halloween costume eithe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>misleading title.</td>\n",
       "      <td>blaming the president for a software deficit. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nah, thats still vegan</td>\n",
       "      <td>it just means it made in the same factory as o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>confusing article. it starts out talk about do...</td>\n",
       "      <td>this entire article is basically the ceo sayin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ag barr just resigned as well.</td>\n",
       "      <td>i hope that biden does well for the same reaso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7995</th>\n",
       "      <td>fresh herbs close to kitchen.</td>\n",
       "      <td>officer: whats that smell? atleast use the sam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7996</th>\n",
       "      <td>years back in ca a driver would use the carpoo...</td>\n",
       "      <td>take it to court! original dallas morning news...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7997</th>\n",
       "      <td>it's only bitcoin, you can always replace it!</td>\n",
       "      <td>this is one time i approve of not holding. all...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7998</th>\n",
       "      <td>this is my hometown. i had to deal with these ...</td>\n",
       "      <td>one girl saw the harry potter book that jessic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7999</th>\n",
       "      <td>some don't like scientists talking re #climate...</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "0     attend a gp like that and see if you can sneak...   \n",
       "1                                    misleading title.    \n",
       "2                               nah, thats still vegan    \n",
       "3     confusing article. it starts out talk about do...   \n",
       "4                       ag barr just resigned as well.    \n",
       "...                                                 ...   \n",
       "7995                     fresh herbs close to kitchen.    \n",
       "7996  years back in ca a driver would use the carpoo...   \n",
       "7997     it's only bitcoin, you can always replace it!    \n",
       "7998  this is my hometown. i had to deal with these ...   \n",
       "7999  some don't like scientists talking re #climate...   \n",
       "\n",
       "                                               comments  \n",
       "0     i dont understand your halloween costume eithe...  \n",
       "1     blaming the president for a software deficit. ...  \n",
       "2     it just means it made in the same factory as o...  \n",
       "3     this entire article is basically the ceo sayin...  \n",
       "4     i hope that biden does well for the same reaso...  \n",
       "...                                                 ...  \n",
       "7995  officer: whats that smell? atleast use the sam...  \n",
       "7996  take it to court! original dallas morning news...  \n",
       "7997  this is one time i approve of not holding. all...  \n",
       "7998  one girl saw the harry potter book that jessic...  \n",
       "7999                                                nan  \n",
       "\n",
       "[8000 rows x 2 columns]"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame()\n",
    "data['text'] = df['nlp_title'] + \" \" + df['nlp_text'].fillna('') \n",
    "data['comments'] = df['nlp_comments'].astype(str)\n",
    "data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# remove punctuations and urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(r'https\\S+', 'https', x))\n",
    "# def remove_urls(text):\n",
    "#     url_pattern = re.compile(r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\")\n",
    "#     return re.sub(url_pattern, '', text)\n",
    "#df['text'] = df['text'].apply(remove_urls)\n",
    "df['text'] = df['text'].apply(lambda x: re.sub('\\[[^]]*\\]', '', x))\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(\"[^a-zA-Z@#]\", ' ', x))\n",
    "df['text'] = df['text'].apply(lambda x: re.sub('\\d', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "df\n",
    "df.to_csv('output.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lemmatize and remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def lemmatize(text):\n",
    "    lemma = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(text)  # Tokenize the text\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmas = [lemma.lemmatize(token) for token in tokens if token.lower() not in stop_words]  # Lemmatize each token\n",
    "    return \" \".join(lemmas)  # Join the lemmatized tokens back into a string\n",
    "\n",
    "df['text'] = df['text'].apply(lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/snowy/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def lemmatize(text):\n",
    "    lemma = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(text)  # Tokenize the text\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    negation_terms = ['not', 'no', 'nobody', 'none', 'neither', 'nowhere', 'never', 'cannot']\n",
    "    lemmas = []\n",
    "    negation_flag = False\n",
    "\n",
    "    for token in tokens:\n",
    "        if negation_flag:\n",
    "            lemmas.append('not_' + lemma.lemmatize(token))\n",
    "        elif token.lower() not in stop_words:\n",
    "            lemmas.append(lemma.lemmatize(token))\n",
    "\n",
    "        if token in negation_terms:\n",
    "            negation_flag = True\n",
    "        elif token in [\".\", \"!\", \"?\"]:\n",
    "            negation_flag = False\n",
    "    \n",
    "    return \" \".join(lemmas)  # Join the lemmatized tokens back into a string\n",
    "\n",
    "df['text'] = df['text'].apply(lemmatize)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEMMING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def stem(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stems = [stemmer.stem(token) for token in tokens if token.lower() not in stop_words]\n",
    "    return \" \".join(stems)\n",
    "\n",
    "df['text'] = df['text'].apply(stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/snowy/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#without lemmatizing\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def remove_stop(text):\n",
    "    tokens = word_tokenize(text)  # Tokenize the text\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    result = [token for token in tokens if token.lower() not in stop_words]  # Lemmatize each token\n",
    "    return \" \".join(result)  # Join the lemmatized tokens back into a string\n",
    "\n",
    "df['text'] = df['text'].apply(remove_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nlp_title</th>\n",
       "      <th>nlp_text</th>\n",
       "      <th>nlp_comments</th>\n",
       "      <th>nlp_class</th>\n",
       "      <th>cv_filename</th>\n",
       "      <th>cv_class</th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>attend a gp like that and see if you can sneak...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>i dont understand your halloween costume eithe...</td>\n",
       "      <td>real</td>\n",
       "      <td>0.jpg</td>\n",
       "      <td>real</td>\n",
       "      <td>1</td>\n",
       "      <td>attend gp like see sneak merc garage dont unde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>misleading title.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>blaming the president for a software deficit. ...</td>\n",
       "      <td>fake</td>\n",
       "      <td>1.jpg</td>\n",
       "      <td>real</td>\n",
       "      <td>0</td>\n",
       "      <td>misleading title blaming president software de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nah, thats still vegan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>it just means it made in the same factory as o...</td>\n",
       "      <td>fake</td>\n",
       "      <td>2.jpg</td>\n",
       "      <td>fake</td>\n",
       "      <td>0</td>\n",
       "      <td>nah thats still vegan mean made factory produc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>confusing article. it starts out talk about do...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>this entire article is basically the ceo sayin...</td>\n",
       "      <td>fake</td>\n",
       "      <td>3.jpg</td>\n",
       "      <td>real</td>\n",
       "      <td>0</td>\n",
       "      <td>confusing article start talk domestic lithium ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ag barr just resigned as well.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>i hope that biden does well for the same reaso...</td>\n",
       "      <td>real</td>\n",
       "      <td>4.jpg</td>\n",
       "      <td>real</td>\n",
       "      <td>1</td>\n",
       "      <td>ag barr resigned well hope biden well reason h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7995</th>\n",
       "      <td>fresh herbs close to kitchen.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>officer: whats that smell? atleast use the sam...</td>\n",
       "      <td>real</td>\n",
       "      <td>7995.jpg</td>\n",
       "      <td>real</td>\n",
       "      <td>1</td>\n",
       "      <td>fresh herb close kitchen officer whats smell a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7996</th>\n",
       "      <td>years back in ca a driver would use the carpoo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>take it to court! original dallas morning news...</td>\n",
       "      <td>real</td>\n",
       "      <td>7996.jpg</td>\n",
       "      <td>real</td>\n",
       "      <td>1</td>\n",
       "      <td>year back ca driver would use carpool lane eve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7997</th>\n",
       "      <td>it's only bitcoin, you can always replace it!</td>\n",
       "      <td>NaN</td>\n",
       "      <td>this is one time i approve of not holding. all...</td>\n",
       "      <td>real</td>\n",
       "      <td>7997.jpg</td>\n",
       "      <td>fake</td>\n",
       "      <td>1</td>\n",
       "      <td>bitcoin always replace one time approve not_ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7998</th>\n",
       "      <td>this is my hometown. i had to deal with these ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>one girl saw the harry potter book that jessic...</td>\n",
       "      <td>real</td>\n",
       "      <td>7998.jpg</td>\n",
       "      <td>real</td>\n",
       "      <td>1</td>\n",
       "      <td>hometown deal bible class little get walk room...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7999</th>\n",
       "      <td>some don't like scientists talking re #climate...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fake</td>\n",
       "      <td>7999.jpg</td>\n",
       "      <td>fake</td>\n",
       "      <td>0</td>\n",
       "      <td>like scientist talking # climate change disast...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8000 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              nlp_title nlp_text  \\\n",
       "0     attend a gp like that and see if you can sneak...      NaN   \n",
       "1                                     misleading title.      NaN   \n",
       "2                                nah, thats still vegan      NaN   \n",
       "3     confusing article. it starts out talk about do...      NaN   \n",
       "4                        ag barr just resigned as well.      NaN   \n",
       "...                                                 ...      ...   \n",
       "7995                      fresh herbs close to kitchen.      NaN   \n",
       "7996  years back in ca a driver would use the carpoo...      NaN   \n",
       "7997      it's only bitcoin, you can always replace it!      NaN   \n",
       "7998  this is my hometown. i had to deal with these ...      NaN   \n",
       "7999  some don't like scientists talking re #climate...      NaN   \n",
       "\n",
       "                                           nlp_comments nlp_class cv_filename  \\\n",
       "0     i dont understand your halloween costume eithe...      real       0.jpg   \n",
       "1     blaming the president for a software deficit. ...      fake       1.jpg   \n",
       "2     it just means it made in the same factory as o...      fake       2.jpg   \n",
       "3     this entire article is basically the ceo sayin...      fake       3.jpg   \n",
       "4     i hope that biden does well for the same reaso...      real       4.jpg   \n",
       "...                                                 ...       ...         ...   \n",
       "7995  officer: whats that smell? atleast use the sam...      real    7995.jpg   \n",
       "7996  take it to court! original dallas morning news...      real    7996.jpg   \n",
       "7997  this is one time i approve of not holding. all...      real    7997.jpg   \n",
       "7998  one girl saw the harry potter book that jessic...      real    7998.jpg   \n",
       "7999                                                NaN      fake    7999.jpg   \n",
       "\n",
       "     cv_class  target                                               text  \n",
       "0        real       1  attend gp like see sneak merc garage dont unde...  \n",
       "1        real       0  misleading title blaming president software de...  \n",
       "2        fake       0  nah thats still vegan mean made factory produc...  \n",
       "3        real       0  confusing article start talk domestic lithium ...  \n",
       "4        real       1  ag barr resigned well hope biden well reason h...  \n",
       "...       ...     ...                                                ...  \n",
       "7995     real       1  fresh herb close kitchen officer whats smell a...  \n",
       "7996     real       1  year back ca driver would use carpool lane eve...  \n",
       "7997     fake       1  bitcoin always replace one time approve not_ho...  \n",
       "7998     real       1  hometown deal bible class little get walk room...  \n",
       "7999     fake       0  like scientist talking # climate change disast...  \n",
       "\n",
       "[8000 rows x 8 columns]"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df.to_csv('output2.csv', index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X, y = df['text'], df['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>attend gp like see sneak merc garage dont unde...</td>\n",
       "      <td>dont understand halloween costume either angel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>misleading title blaming president software de...</td>\n",
       "      <td>blaming president software deficit ok firm dev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nah thats still vegan mean made factory produc...</td>\n",
       "      <td>mean made factory product contain milk cover l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>confusing article start talk domestic lithium ...</td>\n",
       "      <td>entire article basically ceo saying need faste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ag barr resigned well hope biden well reason h...</td>\n",
       "      <td>hope biden well reason hope pilot flying airpl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7995</th>\n",
       "      <td>fresh herb close kitchen officer whats smell a...</td>\n",
       "      <td>officer whats smell atleast use tile rest floo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7996</th>\n",
       "      <td>year back ca driver would use carpool lane eve...</td>\n",
       "      <td>take court original dallas morning news articl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7997</th>\n",
       "      <td>bitcoin always replace one time approve holdin...</td>\n",
       "      <td>one time approve holding love beastie priority...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7998</th>\n",
       "      <td>hometown deal bible class little get walk room...</td>\n",
       "      <td>one girl saw harry potter book jessica roe rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7999</th>\n",
       "      <td>like scientist talking # climate change disast...</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "0     attend gp like see sneak merc garage dont unde...   \n",
       "1     misleading title blaming president software de...   \n",
       "2     nah thats still vegan mean made factory produc...   \n",
       "3     confusing article start talk domestic lithium ...   \n",
       "4     ag barr resigned well hope biden well reason h...   \n",
       "...                                                 ...   \n",
       "7995  fresh herb close kitchen officer whats smell a...   \n",
       "7996  year back ca driver would use carpool lane eve...   \n",
       "7997  bitcoin always replace one time approve holdin...   \n",
       "7998  hometown deal bible class little get walk room...   \n",
       "7999  like scientist talking # climate change disast...   \n",
       "\n",
       "                                               comments  \n",
       "0     dont understand halloween costume either angel...  \n",
       "1     blaming president software deficit ok firm dev...  \n",
       "2     mean made factory product contain milk cover l...  \n",
       "3     entire article basically ceo saying need faste...  \n",
       "4     hope biden well reason hope pilot flying airpl...  \n",
       "...                                                 ...  \n",
       "7995  officer whats smell atleast use tile rest floo...  \n",
       "7996  take court original dallas morning news articl...  \n",
       "7997  one time approve holding love beastie priority...  \n",
       "7998  one girl saw harry potter book jessica roe rea...  \n",
       "7999                                                nan  \n",
       "\n",
       "[8000 rows x 2 columns]"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2 = pd.DataFrame(X)\n",
    "X2['comments'] = data['comments']\n",
    "\n",
    "X2['comments'] = X2['comments'].apply(lambda x: re.sub(r'https\\S+', 'https', x))\n",
    "# def remove_urls(text):\n",
    "#     url_pattern = re.compile(r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\")\n",
    "#     return re.sub(url_pattern, '', text)\n",
    "#df['text'] = df['text'].apply(remove_urls)\n",
    "X2['comments'] = X2['comments'].apply(lambda x: re.sub('\\[[^]]*\\]', '', x))\n",
    "X2['comments'] = X2['comments'].apply(lambda x: re.sub(\"[^a-zA-Z@#]\", ' ', x))\n",
    "X2['comments'] = X2['comments'].apply(lemmatize)\n",
    "\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y, test_size=0.2, random_state=42)\n",
    "X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [2, 6400]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[223], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m tfidf_test3 \u001b[39m=\u001b[39m my_tfidf3\u001b[39m.\u001b[39mtransform(X_test2)\n\u001b[1;32m     17\u001b[0m pa_clf2 \u001b[39m=\u001b[39m PassiveAggressiveClassifier(max_iter\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m pa_clf2\u001b[39m.\u001b[39;49mfit(tfidf_train3, y_train2)\n\u001b[1;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m confusion_matrix\n\u001b[1;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m plot_confusion_matrix\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_passive_aggressive.py:291\u001b[0m, in \u001b[0;36mPassiveAggressiveClassifier.fit\u001b[0;34m(self, X, y, coef_init, intercept_init)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m    290\u001b[0m lr \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpa1\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhinge\u001b[39m\u001b[39m\"\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mpa2\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 291\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(\n\u001b[1;32m    292\u001b[0m     X,\n\u001b[1;32m    293\u001b[0m     y,\n\u001b[1;32m    294\u001b[0m     alpha\u001b[39m=\u001b[39;49m\u001b[39m1.0\u001b[39;49m,\n\u001b[1;32m    295\u001b[0m     C\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mC,\n\u001b[1;32m    296\u001b[0m     loss\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mhinge\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    297\u001b[0m     learning_rate\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    298\u001b[0m     coef_init\u001b[39m=\u001b[39;49mcoef_init,\n\u001b[1;32m    299\u001b[0m     intercept_init\u001b[39m=\u001b[39;49mintercept_init,\n\u001b[1;32m    300\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:677\u001b[0m, in \u001b[0;36mBaseSGDClassifier._fit\u001b[0;34m(self, X, y, alpha, C, loss, learning_rate, coef_init, intercept_init, sample_weight)\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[39m# Clear iteration count for multiple call to fit.\u001b[39;00m\n\u001b[1;32m    675\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mt_ \u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m\n\u001b[0;32m--> 677\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_partial_fit(\n\u001b[1;32m    678\u001b[0m     X,\n\u001b[1;32m    679\u001b[0m     y,\n\u001b[1;32m    680\u001b[0m     alpha,\n\u001b[1;32m    681\u001b[0m     C,\n\u001b[1;32m    682\u001b[0m     loss,\n\u001b[1;32m    683\u001b[0m     learning_rate,\n\u001b[1;32m    684\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_iter,\n\u001b[1;32m    685\u001b[0m     classes,\n\u001b[1;32m    686\u001b[0m     sample_weight,\n\u001b[1;32m    687\u001b[0m     coef_init,\n\u001b[1;32m    688\u001b[0m     intercept_init,\n\u001b[1;32m    689\u001b[0m )\n\u001b[1;32m    691\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    692\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtol \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    693\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtol \u001b[39m>\u001b[39m \u001b[39m-\u001b[39mnp\u001b[39m.\u001b[39minf\n\u001b[1;32m    694\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_iter_ \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_iter\n\u001b[1;32m    695\u001b[0m ):\n\u001b[1;32m    696\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    697\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMaximum number of iteration reached before \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    698\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mconvergence. Consider increasing max_iter to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    699\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mimprove the fit.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    700\u001b[0m         ConvergenceWarning,\n\u001b[1;32m    701\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:572\u001b[0m, in \u001b[0;36mBaseSGDClassifier._partial_fit\u001b[0;34m(self, X, y, alpha, C, loss, learning_rate, max_iter, classes, sample_weight, coef_init, intercept_init)\u001b[0m\n\u001b[1;32m    557\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_partial_fit\u001b[39m(\n\u001b[1;32m    558\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    559\u001b[0m     X,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    569\u001b[0m     intercept_init,\n\u001b[1;32m    570\u001b[0m ):\n\u001b[1;32m    571\u001b[0m     first_call \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mclasses_\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 572\u001b[0m     X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[1;32m    573\u001b[0m         X,\n\u001b[1;32m    574\u001b[0m         y,\n\u001b[1;32m    575\u001b[0m         accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    576\u001b[0m         dtype\u001b[39m=\u001b[39;49mnp\u001b[39m.\u001b[39;49mfloat64,\n\u001b[1;32m    577\u001b[0m         order\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mC\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    578\u001b[0m         accept_large_sparse\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    579\u001b[0m         reset\u001b[39m=\u001b[39;49mfirst_call,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[1;32m    582\u001b[0m     n_samples, n_features \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mshape\n\u001b[1;32m    584\u001b[0m     _check_partial_fit_first_call(\u001b[39mself\u001b[39m, classes)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/sklearn/base.py:581\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    579\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[1;32m    580\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 581\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[1;32m    582\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[1;32m    584\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/sklearn/utils/validation.py:981\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m    964\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[1;32m    965\u001b[0m     X,\n\u001b[1;32m    966\u001b[0m     accept_sparse\u001b[39m=\u001b[39maccept_sparse,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    976\u001b[0m     estimator\u001b[39m=\u001b[39mestimator,\n\u001b[1;32m    977\u001b[0m )\n\u001b[1;32m    979\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39mmulti_output, y_numeric\u001b[39m=\u001b[39my_numeric)\n\u001b[0;32m--> 981\u001b[0m check_consistent_length(X, y)\n\u001b[1;32m    983\u001b[0m \u001b[39mreturn\u001b[39;00m X, y\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/sklearn/utils/validation.py:332\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    330\u001b[0m uniques \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(lengths)\n\u001b[1;32m    331\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(uniques) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 332\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    333\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    334\u001b[0m         \u001b[39m%\u001b[39m [\u001b[39mint\u001b[39m(l) \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m lengths]\n\u001b[1;32m    335\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [2, 6400]"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# my_tfidf = TfidfVectorizer(stop_words='english', max_df=0.7)\n",
    "# # fit the vectorizer and transform X_train into a tf-idf matrix,\n",
    "# # then use the same vectorizer to transform X_test\n",
    "# tfidf_train2 = my_tfidf.fit_transform(X_train2)\n",
    "# tfidf_test2 = my_tfidf.fit_transform(X_test2)\n",
    "\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "\n",
    "my_tfidf3 = TfidfVectorizer(stop_words='english', max_df=0.7) #0.7\n",
    "# fit the vectorizer and transform X_train into a tf-idf matrix,\n",
    "# then use the same vectorizer to transform X_test\n",
    "tfidf_train3 = my_tfidf3.fit_transform(X_train2)\n",
    "tfidf_test3 = my_tfidf3.transform(X_test2)\n",
    "\n",
    "pa_clf2 = PassiveAggressiveClassifier(max_iter=50)\n",
    "pa_clf2.fit(tfidf_train3, y_train2)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "pa_pred2 = pa_clf2.predict(tfidf_test3)\n",
    "#pa_pred = pa_clf.predict(roberta_test)\n",
    "\n",
    "pa_score2 = accuracy_score(y_test2, pa_pred2)\n",
    "pa_roc2 = roc_auc_score(y_test2, pa_pred2)\n",
    "print(\"The accuracy of PA is: %0.2f\" %pa_score2)\n",
    "print(\"The roc_auc score of PA is: %0.2f\" %pa_roc2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBOOST with ROBERTA tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "converting documents to features: 100%|██████████| 6400/6400 [00:26<00:00, 240.08it/s]\n",
      "converting documents to features: 100%|██████████| 1600/1600 [00:06<00:00, 247.56it/s]\n"
     ]
    }
   ],
   "source": [
    "roberta_tokenizer = transformers.RobertaTokenizer.from_pretrained('roberta-base-openai-detector')\n",
    "roberta_train_features = func_tokenizer(roberta_tokenizer, X_train)\n",
    "roberta_test_features = func_tokenizer(roberta_tokenizer, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_trg = sequence.pad_sequences(roberta_train_features, maxlen = 500)\n",
    "roberta_test = sequence.pad_sequences(roberta_test_features, maxlen = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric='auc', feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.15, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=9, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=1000, n_jobs=None, num_parallel_tree=None,\n",
       "              predictor=None, random_state=None, ...)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning)\n",
    "from xgboost import XGBClassifier\n",
    "xgb = XGBClassifier(n_estimators = 1000, learning_rate = 0.15, max_depth = 9,\n",
    "                    eval_metric = 'auc', use_label_encoder=False,objective = 'binary:logistic')\n",
    "xgb.fit(roberta_trg, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of XGBOOST is: 0.76\n",
      "The roc_auc score of XGBOOST is: 0.72\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "xgb_pred = xgb.predict(roberta_test)\n",
    "xgb_score = accuracy_score(y_test, xgb_pred)\n",
    "xgb_roc = roc_auc_score(y_test, xgb_pred)\n",
    "print(\"The accuracy of XGBOOST is: %0.2f\" %xgb_score)\n",
    "print(\"The roc_auc score of XGBOOST is: %0.2f\" %xgb_roc)\n",
    "\n",
    "#results before lemminization and removing stop words\n",
    "# The accuracy of XGBOOST is: 0.74\n",
    "# The roc_auc score of XGBOOST is: 0.69"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[216], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39m# Perform grid search to find best hyperparameters\u001b[39;00m\n\u001b[1;32m     15\u001b[0m grid_search \u001b[39m=\u001b[39m GridSearchCV(estimator\u001b[39m=\u001b[39mxgb_model, param_grid\u001b[39m=\u001b[39mparam_grid, cv\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m grid_search\u001b[39m.\u001b[39;49mfit(roberta_trg, y_train)\n\u001b[1;32m     17\u001b[0m xgb_best \u001b[39m=\u001b[39m XGBClassifier(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mgrid_search\u001b[39m.\u001b[39mbest_params_)\n\u001b[1;32m     18\u001b[0m xgb_best\u001b[39m.\u001b[39mfit(roberta_trg, y_train)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/sklearn/model_selection/_search.py:891\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    885\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[1;32m    886\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    887\u001b[0m     )\n\u001b[1;32m    889\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[0;32m--> 891\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[1;32m    893\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    894\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    895\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/sklearn/model_selection/_search.py:1392\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1391\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1392\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_grid))\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/sklearn/model_selection/_search.py:838\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    831\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[1;32m    832\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    833\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    834\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[1;32m    835\u001b[0m         )\n\u001b[1;32m    836\u001b[0m     )\n\u001b[0;32m--> 838\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[1;32m    839\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    840\u001b[0m         clone(base_estimator),\n\u001b[1;32m    841\u001b[0m         X,\n\u001b[1;32m    842\u001b[0m         y,\n\u001b[1;32m    843\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[1;32m    844\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[1;32m    845\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[1;32m    846\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[1;32m    847\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[1;32m    848\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[1;32m    849\u001b[0m     )\n\u001b[1;32m    850\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[1;32m    851\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[1;32m    852\u001b[0m     )\n\u001b[1;32m    853\u001b[0m )\n\u001b[1;32m    855\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    856\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    857\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    858\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    859\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    860\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/joblib/parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1085\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[1;32m   1086\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1088\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[1;32m   1089\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m   1091\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1092\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[1;32m   1093\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[1;32m   1094\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/joblib/parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    899\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 901\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[1;32m    902\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/joblib/parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    818\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[0;32m--> 819\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[1;32m    820\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    821\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    823\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    824\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[1;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/joblib/_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[1;32m    595\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    596\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 597\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/sklearn/utils/fixes.py:216\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    215\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig):\n\u001b[0;32m--> 216\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:680\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    678\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[1;32m    679\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 680\u001b[0m         estimator\u001b[39m.\u001b[39;49mfit(X_train, y_train, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n\u001b[1;32m    682\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m    683\u001b[0m     \u001b[39m# Note fit time as time until error\u001b[39;00m\n\u001b[1;32m    684\u001b[0m     fit_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/xgboost/core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[1;32m    619\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[0;32m--> 620\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/xgboost/sklearn.py:1490\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m   1462\u001b[0m (\n\u001b[1;32m   1463\u001b[0m     model,\n\u001b[1;32m   1464\u001b[0m     metric,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1469\u001b[0m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001b[1;32m   1470\u001b[0m )\n\u001b[1;32m   1471\u001b[0m train_dmatrix, evals \u001b[39m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[1;32m   1472\u001b[0m     missing\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmissing,\n\u001b[1;32m   1473\u001b[0m     X\u001b[39m=\u001b[39mX,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1487\u001b[0m     feature_types\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_types,\n\u001b[1;32m   1488\u001b[0m )\n\u001b[0;32m-> 1490\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_Booster \u001b[39m=\u001b[39m train(\n\u001b[1;32m   1491\u001b[0m     params,\n\u001b[1;32m   1492\u001b[0m     train_dmatrix,\n\u001b[1;32m   1493\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_num_boosting_rounds(),\n\u001b[1;32m   1494\u001b[0m     evals\u001b[39m=\u001b[39;49mevals,\n\u001b[1;32m   1495\u001b[0m     early_stopping_rounds\u001b[39m=\u001b[39;49mearly_stopping_rounds,\n\u001b[1;32m   1496\u001b[0m     evals_result\u001b[39m=\u001b[39;49mevals_result,\n\u001b[1;32m   1497\u001b[0m     obj\u001b[39m=\u001b[39;49mobj,\n\u001b[1;32m   1498\u001b[0m     custom_metric\u001b[39m=\u001b[39;49mmetric,\n\u001b[1;32m   1499\u001b[0m     verbose_eval\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   1500\u001b[0m     xgb_model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m   1501\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m   1502\u001b[0m )\n\u001b[1;32m   1504\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mcallable\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjective):\n\u001b[1;32m   1505\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjective \u001b[39m=\u001b[39m params[\u001b[39m\"\u001b[39m\u001b[39mobjective\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/xgboost/core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[1;32m    619\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[0;32m--> 620\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/xgboost/training.py:185\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[39mif\u001b[39;00m cb_container\u001b[39m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    184\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m bst\u001b[39m.\u001b[39;49mupdate(dtrain, i, obj)\n\u001b[1;32m    186\u001b[0m \u001b[39mif\u001b[39;00m cb_container\u001b[39m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    187\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/xgboost/core.py:1918\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1915\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_dmatrix_features(dtrain)\n\u001b[1;32m   1917\u001b[0m \u001b[39mif\u001b[39;00m fobj \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1918\u001b[0m     _check_call(_LIB\u001b[39m.\u001b[39;49mXGBoosterUpdateOneIter(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle,\n\u001b[1;32m   1919\u001b[0m                                             ctypes\u001b[39m.\u001b[39;49mc_int(iteration),\n\u001b[1;32m   1920\u001b[0m                                             dtrain\u001b[39m.\u001b[39;49mhandle))\n\u001b[1;32m   1921\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1922\u001b[0m     pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict(dtrain, output_margin\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#after hyperparameter tuning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "xgb_model = XGBClassifier()\n",
    "param_grid = {\n",
    "    'learning_rate': [0.1, 0.2, 0.3],\n",
    "    'max_depth': [3, 5, 10, 15],\n",
    "    'n_estimators': [50, 100, 150, 500],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'min_child_weight': [1, 2, 3]\n",
    "}\n",
    "\n",
    "# Perform grid search to find best hyperparameters\n",
    "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(roberta_trg, y_train)\n",
    "xgb_best = XGBClassifier(**grid_search.best_params_)\n",
    "xgb_best.fit(roberta_trg, y_train)\n",
    "\n",
    "xgb_pred2 = xgb_best.predict(roberta_test)\n",
    "xgb_score2 = accuracy_score(y_test, xgb_pred2)\n",
    "xgb_roc2 = roc_auc_score(y_test, xgb_pred2)\n",
    "print(\"The accuracy of XGBOOST is: %0.2f\" %xgb_score2)\n",
    "print(\"The roc_auc score of XGBOOST is: %0.2f\" %xgb_roc2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAT BOOST with ROBERTA tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "cb = CatBoostClassifier(eval_metric = 'Accuracy', iterations = 2000, learning_rate = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of CatBoost is: 0.77\n",
      "The roc_auc score of CatBoost is: 0.73\n"
     ]
    }
   ],
   "source": [
    "cb.fit(roberta_trg, y_train, verbose = 0)\n",
    "cb_pred = cb.predict(roberta_test)\n",
    "cb_score = accuracy_score(y_test, cb_pred)\n",
    "cb_roc = roc_auc_score(y_test, cb_pred)\n",
    "print(\"The accuracy of CatBoost is: %0.2f\" %cb_score)\n",
    "print(\"The roc_auc score of CatBoost is: %0.2f\" %cb_roc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PASSIVE AGGRESSIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<6400x115694 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 2777159 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "my_tfidf = TfidfVectorizer(stop_words='english', max_df=0.7) #0.7\n",
    "# fit the vectorizer and transform X_train into a tf-idf matrix,\n",
    "# then use the same vectorizer to transform X_test\n",
    "tfidf_train = my_tfidf.fit_transform(X_train)\n",
    "tfidf_test = my_tfidf.transform(X_test)\n",
    "\n",
    "tfidf_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassiveAggressiveClassifier(C=100, max_iter=200, random_state=10)"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "\n",
    "pa_clf = PassiveAggressiveClassifier(max_iter=200, C = 100, random_state = 10) \n",
    "pa_clf.fit(tfidf_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of PA is: 0.86\n",
      "The roc_auc score of PA is: 0.85\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "pa_pred = pa_clf.predict(tfidf_test)\n",
    "#pa_pred = pa_clf.predict(roberta_test)\n",
    "\n",
    "pa_score = accuracy_score(y_test, pa_pred)\n",
    "pa_roc = roc_auc_score(y_test, pa_pred)\n",
    "print(\"The accuracy of PA is: %0.2f\" %pa_score)\n",
    "print(\"The roc_auc score of PA is: %0.2f\" %pa_roc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'C': 10, 'loss': 'squared_hinge', 'max_iter': 1000, 'random_state': 42}\n",
      "Best Accuracy: 0.85375\n",
      "Training Accuracy: 0.85625\n",
      "Training ROC-AUC: 0.8463260086507912\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'max_iter': [1000, 2000, 3000],\n",
    "    'loss': ['hinge', 'squared_hinge'],\n",
    "    'random_state': [42] \n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=pa_clf, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(tfidf_train, y_train)\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Accuracy:\", grid_search.best_score_)\n",
    "\n",
    "# Train and evaluate the model\n",
    "pa_clf = grid_search.best_estimator_\n",
    "pa_clf.fit(tfidf_train, y_train)\n",
    "pa_pred = pa_clf.predict(tfidf_test)\n",
    "pa_score = accuracy_score(y_test, pa_pred)\n",
    "pa_roc = roc_auc_score(y_test, pa_pred)\n",
    "print(\"Training Accuracy:\", pa_score)\n",
    "print(\"Training ROC-AUC:\", pa_roc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRE-PROCESS TEST SET "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/snowy/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nlp_title</th>\n",
       "      <th>nlp_text</th>\n",
       "      <th>nlp_comments</th>\n",
       "      <th>cv_filename</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>so a***ton of cp and multiple illegal weapons ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>the man worked as a director on some bbc and c...</td>\n",
       "      <td>0.jpg</td>\n",
       "      <td>ton cp multiple illegal weapon dosent get jail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>demo of the average brits understanding of 80k...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>lots of people don't seem to understand that t...</td>\n",
       "      <td>1.jpg</td>\n",
       "      <td>demo average brit understanding k top income l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>please do adobe next.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>if you can join something with a couple of cli...</td>\n",
       "      <td>2.jpg</td>\n",
       "      <td>please adobe next join something couple click ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>private companies that spend billions on lobby...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yup california had a free online tax filing sy...</td>\n",
       "      <td>3.jpg</td>\n",
       "      <td>private company spend billion lobbying suppose...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>he is dead. his name was len sassaman</td>\n",
       "      <td>peope say he might sell his 1m coins one day b...</td>\n",
       "      <td>he might sell his 1m coins one day but dont y...</td>\n",
       "      <td>4.jpg</td>\n",
       "      <td>dead name len sassaman peope say might sell co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>seeing it came from rt i did a little research...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>people should just stop killing dogs altogethe...</td>\n",
       "      <td>1995.jpg</td>\n",
       "      <td>seeing came rt little research got year based ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>rt @scifri: what questions do you have about u...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1996.jpg</td>\n",
       "      <td>rt @ scifri question using geoengineering comb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>he shouldnt be embarrassed, he was great in lotr.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fun fact about this ordeal : the man responsib...</td>\n",
       "      <td>1997.jpg</td>\n",
       "      <td>shouldnt embarrassed great lotr fun fact ordea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>this is an atrociously bad photoshop... that's...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>it took me way too long to realize data was th...</td>\n",
       "      <td>1998.jpg</td>\n",
       "      <td>atrociously bad photoshop not_his not_body not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>rotterdam? geography much?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>rotterdam and rotherham are two different plac...</td>\n",
       "      <td>1999.jpg</td>\n",
       "      <td>rotterdam geography much rotterdam rotherham t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              nlp_title  \\\n",
       "0     so a***ton of cp and multiple illegal weapons ...   \n",
       "1     demo of the average brits understanding of 80k...   \n",
       "2                                 please do adobe next.   \n",
       "3     private companies that spend billions on lobby...   \n",
       "4                 he is dead. his name was len sassaman   \n",
       "...                                                 ...   \n",
       "1995  seeing it came from rt i did a little research...   \n",
       "1996  rt @scifri: what questions do you have about u...   \n",
       "1997  he shouldnt be embarrassed, he was great in lotr.   \n",
       "1998  this is an atrociously bad photoshop... that's...   \n",
       "1999                         rotterdam? geography much?   \n",
       "\n",
       "                                               nlp_text  \\\n",
       "0                                                   NaN   \n",
       "1                                                   NaN   \n",
       "2                                                   NaN   \n",
       "3                                                   NaN   \n",
       "4     peope say he might sell his 1m coins one day b...   \n",
       "...                                                 ...   \n",
       "1995                                                NaN   \n",
       "1996                                                NaN   \n",
       "1997                                                NaN   \n",
       "1998                                                NaN   \n",
       "1999                                                NaN   \n",
       "\n",
       "                                           nlp_comments cv_filename  \\\n",
       "0     the man worked as a director on some bbc and c...       0.jpg   \n",
       "1     lots of people don't seem to understand that t...       1.jpg   \n",
       "2     if you can join something with a couple of cli...       2.jpg   \n",
       "3     yup california had a free online tax filing sy...       3.jpg   \n",
       "4      he might sell his 1m coins one day but dont y...       4.jpg   \n",
       "...                                                 ...         ...   \n",
       "1995  people should just stop killing dogs altogethe...    1995.jpg   \n",
       "1996                                                NaN    1996.jpg   \n",
       "1997  fun fact about this ordeal : the man responsib...    1997.jpg   \n",
       "1998  it took me way too long to realize data was th...    1998.jpg   \n",
       "1999  rotterdam and rotherham are two different plac...    1999.jpg   \n",
       "\n",
       "                                                   text  \n",
       "0     ton cp multiple illegal weapon dosent get jail...  \n",
       "1     demo average brit understanding k top income l...  \n",
       "2     please adobe next join something couple click ...  \n",
       "3     private company spend billion lobbying suppose...  \n",
       "4     dead name len sassaman peope say might sell co...  \n",
       "...                                                 ...  \n",
       "1995  seeing came rt little research got year based ...  \n",
       "1996  rt @ scifri question using geoengineering comb...  \n",
       "1997  shouldnt embarrassed great lotr fun fact ordea...  \n",
       "1998  atrociously bad photoshop not_his not_body not...  \n",
       "1999  rotterdam geography much rotterdam rotherham t...  \n",
       "\n",
       "[2000 rows x 5 columns]"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re \n",
    "\n",
    "testset = pd.read_csv(\"test.csv\")\n",
    "testset['text'] = testset['nlp_title'] + \" \" + testset['nlp_text'].fillna('') + \" \" + testset['nlp_comments'].astype(str)\n",
    "\n",
    "testset['text'] = testset['text'].apply(lambda x: re.sub(r'https\\S+', 'https', x))\n",
    "# def remove_urls(text):\n",
    "#     url_pattern = re.compile(r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\")\n",
    "#     return re.sub(url_pattern, '', text)\n",
    "#df['text'] = df['text'].apply(remove_urls)\n",
    "testset['text'] = testset['text'].apply(lambda x: re.sub('\\[[^]]*\\]', '', x))\n",
    "testset['text'] = testset['text'].apply(lambda x: re.sub(\"[^a-zA-Z@#]\", ' ', x))\n",
    "testset['text'] = testset['text'].apply(lambda x: re.sub('\\d', '', x))\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def lemmatize(text):\n",
    "    lemma = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(text)  # Tokenize the text\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    negation_terms = ['not', 'no', 'nobody', 'none', 'neither', 'nowhere', 'never', 'cannot']\n",
    "    lemmas = []\n",
    "    negation_flag = False\n",
    "\n",
    "    for token in tokens:\n",
    "        if negation_flag:\n",
    "            lemmas.append('not_' + lemma.lemmatize(token))\n",
    "        elif token.lower() not in stop_words:\n",
    "            lemmas.append(lemma.lemmatize(token))\n",
    "\n",
    "        if token in negation_terms:\n",
    "            negation_flag = True\n",
    "        elif token in [\".\", \"!\", \"?\"]:\n",
    "            negation_flag = False\n",
    "    \n",
    "    return \" \".join(lemmas)  # Join the lemmatized tokens back into a string\n",
    "\n",
    "testset['text'] = testset['text'].apply(lemmatize)\n",
    "\n",
    "testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "#my_tfidf = TfidfVectorizer(stop_words='english', max_df=0.7) #0.7\n",
    "\n",
    "testing = testset['text']\n",
    "\n",
    "tfidf_testset = my_tfidf.transform(testing)\n",
    "\n",
    "tfidf_testset\n",
    "\n",
    "# pa_clf3 = PassiveAggressiveClassifier(max_iter=200, C = 100, random_state = 10) \n",
    "# pa_clf3.fit(tfidf_train, y_train)\n",
    "\n",
    "results = pa_clf.predict(tfidf_testset)\n",
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 1, 1])"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(results).to_csv('nlp_result.csv',index= False )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of PA is: 0.85\n",
      "The roc_auc score of PA is: 0.83\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC()\n",
    "svm.fit(tfidf_train, y_train)\n",
    "\n",
    "svm_pred = svm.predict(tfidf_test)\n",
    "svm_score = accuracy_score(y_test, svm_pred)\n",
    "svm_roc = roc_auc_score(y_test, svm_pred)\n",
    "\n",
    "print(\"The accuracy of SVM is: %0.2f\" %svm_score)\n",
    "print(\"The roc_auc score of SVM is: %0.2f\" %svm_roc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_test = testset['text']\n",
    "svm_testset = my_tfidf.transform(svm_test)\n",
    "\n",
    "svm_preds = svm.predict(svm_testset)\n",
    "svm_preds\n",
    "pd.Series(svm_preds).to_csv('svm_predictions.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADABOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of PA is: 0.86\n",
      "The roc_auc score of PA is: 0.85\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "adaboost = AdaBoostClassifier()\n",
    "adaboost.fit(tfidf_train, y_train)\n",
    "\n",
    "ada_pred = adaboost.predict(tfidf_test)\n",
    "ada_score = accuracy_score(y_test, ada_pred)\n",
    "ada_roc = roc_auc_score(y_test, ada_pred)\n",
    "\n",
    "print(\"The accuracy of PA is: %0.2f\" %ada_score)\n",
    "print(\"The roc_auc score of PA is: %0.2f\" %ada_roc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 1, 1])"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada_test = testset['text']\n",
    "ada_testset = my_tfidf.transform(ada_test)\n",
    "\n",
    "ada_preds = adaboost.predict(ada_testset)\n",
    "ada_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(ada_preds).to_csv('nlp_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<6400x93223 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 2227342 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nlp_title</th>\n",
       "      <th>nlp_text</th>\n",
       "      <th>nlp_comments</th>\n",
       "      <th>cv_filename</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>so a***ton of cp and multiple illegal weapons ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>the man worked as a director on some bbc and c...</td>\n",
       "      <td>0.jpg</td>\n",
       "      <td>ton cp multiple illegal weapon dosent get jail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>demo of the average brits understanding of 80k...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>lots of people don't seem to understand that t...</td>\n",
       "      <td>1.jpg</td>\n",
       "      <td>demo average brit understanding k top income l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>please do adobe next.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>if you can join something with a couple of cli...</td>\n",
       "      <td>2.jpg</td>\n",
       "      <td>please adobe next join something couple click ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>private companies that spend billions on lobby...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yup california had a free online tax filing sy...</td>\n",
       "      <td>3.jpg</td>\n",
       "      <td>private company spend billion lobbying suppose...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>he is dead. his name was len sassaman</td>\n",
       "      <td>peope say he might sell his 1m coins one day b...</td>\n",
       "      <td>he might sell his 1m coins one day but dont y...</td>\n",
       "      <td>4.jpg</td>\n",
       "      <td>dead name len sassaman peope say might sell co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>seeing it came from rt i did a little research...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>people should just stop killing dogs altogethe...</td>\n",
       "      <td>1995.jpg</td>\n",
       "      <td>seeing came rt little research got year based ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>rt @scifri: what questions do you have about u...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1996.jpg</td>\n",
       "      <td>rt @ scifri question using geoengineering comb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>he shouldnt be embarrassed, he was great in lotr.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fun fact about this ordeal : the man responsib...</td>\n",
       "      <td>1997.jpg</td>\n",
       "      <td>shouldnt embarrassed great lotr fun fact ordea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>this is an atrociously bad photoshop... that's...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>it took me way too long to realize data was th...</td>\n",
       "      <td>1998.jpg</td>\n",
       "      <td>atrociously bad photoshop body still see shitt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>rotterdam? geography much?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>rotterdam and rotherham are two different plac...</td>\n",
       "      <td>1999.jpg</td>\n",
       "      <td>rotterdam geography much rotterdam rotherham t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              nlp_title  \\\n",
       "0     so a***ton of cp and multiple illegal weapons ...   \n",
       "1     demo of the average brits understanding of 80k...   \n",
       "2                                 please do adobe next.   \n",
       "3     private companies that spend billions on lobby...   \n",
       "4                 he is dead. his name was len sassaman   \n",
       "...                                                 ...   \n",
       "1995  seeing it came from rt i did a little research...   \n",
       "1996  rt @scifri: what questions do you have about u...   \n",
       "1997  he shouldnt be embarrassed, he was great in lotr.   \n",
       "1998  this is an atrociously bad photoshop... that's...   \n",
       "1999                         rotterdam? geography much?   \n",
       "\n",
       "                                               nlp_text  \\\n",
       "0                                                   NaN   \n",
       "1                                                   NaN   \n",
       "2                                                   NaN   \n",
       "3                                                   NaN   \n",
       "4     peope say he might sell his 1m coins one day b...   \n",
       "...                                                 ...   \n",
       "1995                                                NaN   \n",
       "1996                                                NaN   \n",
       "1997                                                NaN   \n",
       "1998                                                NaN   \n",
       "1999                                                NaN   \n",
       "\n",
       "                                           nlp_comments cv_filename  \\\n",
       "0     the man worked as a director on some bbc and c...       0.jpg   \n",
       "1     lots of people don't seem to understand that t...       1.jpg   \n",
       "2     if you can join something with a couple of cli...       2.jpg   \n",
       "3     yup california had a free online tax filing sy...       3.jpg   \n",
       "4      he might sell his 1m coins one day but dont y...       4.jpg   \n",
       "...                                                 ...         ...   \n",
       "1995  people should just stop killing dogs altogethe...    1995.jpg   \n",
       "1996                                                NaN    1996.jpg   \n",
       "1997  fun fact about this ordeal : the man responsib...    1997.jpg   \n",
       "1998  it took me way too long to realize data was th...    1998.jpg   \n",
       "1999  rotterdam and rotherham are two different plac...    1999.jpg   \n",
       "\n",
       "                                                   text  \n",
       "0     ton cp multiple illegal weapon dosent get jail...  \n",
       "1     demo average brit understanding k top income l...  \n",
       "2     please adobe next join something couple click ...  \n",
       "3     private company spend billion lobbying suppose...  \n",
       "4     dead name len sassaman peope say might sell co...  \n",
       "...                                                 ...  \n",
       "1995  seeing came rt little research got year based ...  \n",
       "1996  rt @ scifri question using geoengineering comb...  \n",
       "1997  shouldnt embarrassed great lotr fun fact ordea...  \n",
       "1998  atrociously bad photoshop body still see shitt...  \n",
       "1999  rotterdam geography much rotterdam rotherham t...  \n",
       "\n",
       "[2000 rows x 5 columns]"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "max_features = 10000\n",
    "maxlen = 150\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "tokenized_train = tokenizer.texts_to_sequences(X_train)\n",
    "x_train = sequence.pad_sequences(tokenized_train, maxlen=maxlen)\n",
    "\n",
    "tokenized_test = tokenizer.texts_to_sequences(X_test)\n",
    "x_test = sequence.pad_sequences(tokenized_test, maxlen=maxlen)\n",
    "\n",
    "batch_size = 256\n",
    "epochs = 10\n",
    "embed_size = 100\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience = 2, verbose=1,factor=0.5, min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import Dense,Embedding,LSTM\n",
    "from keras.models import Sequential\n",
    "\n",
    "#Defining Neural Network\n",
    "model = Sequential()\n",
    "#Non-trainable embeddidng layer\n",
    "model.add(Embedding(max_features, output_dim=embed_size, input_length=maxlen, trainable=False)) #weights=[embedding_matrix]\n",
    "#LSTM \n",
    "model.add(LSTM(units=128 , return_sequences = True , recurrent_dropout = 0.25 , dropout = 0.25))\n",
    "model.add(LSTM(units=64 , recurrent_dropout = 0.1 , dropout = 0.1))\n",
    "model.add(Dense(units = 32 , activation = 'relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer= keras.optimizers.Adam(lr = 0.01), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.5333 - accuracy: 0.7391"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-22 12:42:43.947682: W tensorflow/core/framework/op_kernel.cc:1805] OP_REQUIRES failed at cast_op.cc:121 : UNIMPLEMENTED: Cast string to float is not supported\n"
     ]
    },
    {
     "ename": "UnimplementedError",
     "evalue": "Graph execution error:\n\nDetected at node 'sequential_1/Cast' defined at (most recent call last):\n    File \"/opt/homebrew/Cellar/python@3.9/3.9.10/Frameworks/Python.framework/Versions/3.9/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/opt/homebrew/Cellar/python@3.9/3.9.10/Frameworks/Python.framework/Versions/3.9/lib/python3.9/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/Users/snowy/Library/Python/3.9/lib/python/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/Users/snowy/Library/Python/3.9/lib/python/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n      app.start()\n    File \"/Users/snowy/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelapp.py\", line 725, in start\n      self.io_loop.start()\n    File \"/Users/snowy/Library/Python/3.9/lib/python/site-packages/tornado/platform/asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"/opt/homebrew/Cellar/python@3.9/3.9.10/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\", line 596, in run_forever\n      self._run_once()\n    File \"/opt/homebrew/Cellar/python@3.9/3.9.10/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\", line 1890, in _run_once\n      handle._run()\n    File \"/opt/homebrew/Cellar/python@3.9/3.9.10/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/Users/snowy/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelbase.py\", line 513, in dispatch_queue\n      await self.process_one()\n    File \"/Users/snowy/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelbase.py\", line 502, in process_one\n      await dispatch(*args)\n    File \"/Users/snowy/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelbase.py\", line 409, in dispatch_shell\n      await result\n    File \"/Users/snowy/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelbase.py\", line 729, in execute_request\n      reply_content = await reply_content\n    File \"/Users/snowy/Library/Python/3.9/lib/python/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n      res = shell.run_cell(\n    File \"/Users/snowy/Library/Python/3.9/lib/python/site-packages/ipykernel/zmqshell.py\", line 540, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/Users/snowy/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_cell\n      result = self._run_cell(\n    File \"/Users/snowy/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3061, in _run_cell\n      result = runner(coro)\n    File \"/Users/snowy/Library/Python/3.9/lib/python/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/Users/snowy/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3266, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/Users/snowy/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3445, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/Users/snowy/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/var/folders/yp/_xstfn5s2nb1lmynh4_4ghmc0000gn/T/ipykernel_31675/3604603000.py\", line 1, in <module>\n      history = model.fit(x_train, y_train, batch_size = batch_size , validation_data = (X_test,y_test) , epochs = epochs , callbacks = [learning_rate_reduction])\n    File \"/opt/homebrew/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/opt/homebrew/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1791, in fit\n      val_logs = self.evaluate(\n    File \"/opt/homebrew/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/opt/homebrew/lib/python3.9/site-packages/keras/src/engine/training.py\", line 2200, in evaluate\n      logs = test_function_runner.run_step(\n    File \"/opt/homebrew/lib/python3.9/site-packages/keras/src/engine/training.py\", line 4000, in run_step\n      tmp_logs = self._function(dataset_or_iterator)\n    File \"/opt/homebrew/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1972, in test_function\n      return step_function(self, iterator)\n    File \"/opt/homebrew/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1956, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/homebrew/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1944, in run_step\n      outputs = model.test_step(data)\n    File \"/opt/homebrew/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1850, in test_step\n      y_pred = self(x, training=False)\n    File \"/opt/homebrew/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/opt/homebrew/lib/python3.9/site-packages/keras/src/engine/training.py\", line 569, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/opt/homebrew/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/opt/homebrew/lib/python3.9/site-packages/keras/src/engine/base_layer.py\", line 1150, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/opt/homebrew/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/opt/homebrew/lib/python3.9/site-packages/keras/src/engine/sequential.py\", line 404, in call\n      return super().call(inputs, training=training, mask=mask)\n    File \"/opt/homebrew/lib/python3.9/site-packages/keras/src/engine/functional.py\", line 512, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"/opt/homebrew/lib/python3.9/site-packages/keras/src/engine/functional.py\", line 651, in _run_internal_graph\n      y = self._conform_to_reference_input(y, ref_input=x)\n    File \"/opt/homebrew/lib/python3.9/site-packages/keras/src/engine/functional.py\", line 748, in _conform_to_reference_input\n      tensor = tf.cast(tensor, dtype=ref_input.dtype)\nNode: 'sequential_1/Cast'\nCast string to float is not supported\n\t [[{{node sequential_1/Cast}}]] [Op:__inference_test_function_6205]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnimplementedError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[330], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(x_train, y_train, batch_size \u001b[39m=\u001b[39;49m batch_size , validation_data \u001b[39m=\u001b[39;49m (X_test,y_test) , epochs \u001b[39m=\u001b[39;49m epochs , callbacks \u001b[39m=\u001b[39;49m [learning_rate_reduction])\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mAccuracy of the model on Training Data is - \u001b[39m\u001b[39m\"\u001b[39m , model\u001b[39m.\u001b[39mevaluate(x_train,y_train)[\u001b[39m1\u001b[39m]\u001b[39m*\u001b[39m\u001b[39m100\u001b[39m , \u001b[39m\"\u001b[39m\u001b[39m%\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mAccuracy of the model on Testing Data is - \u001b[39m\u001b[39m\"\u001b[39m , model\u001b[39m.\u001b[39mevaluate(X_test,y_test)[\u001b[39m1\u001b[39m]\u001b[39m*\u001b[39m\u001b[39m100\u001b[39m , \u001b[39m\"\u001b[39m\u001b[39m%\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mUnimplementedError\u001b[0m: Graph execution error:\n\nDetected at node 'sequential_1/Cast' defined at (most recent call last):\n    File \"/opt/homebrew/Cellar/python@3.9/3.9.10/Frameworks/Python.framework/Versions/3.9/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/opt/homebrew/Cellar/python@3.9/3.9.10/Frameworks/Python.framework/Versions/3.9/lib/python3.9/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/Users/snowy/Library/Python/3.9/lib/python/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/Users/snowy/Library/Python/3.9/lib/python/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n      app.start()\n    File \"/Users/snowy/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelapp.py\", line 725, in start\n      self.io_loop.start()\n    File \"/Users/snowy/Library/Python/3.9/lib/python/site-packages/tornado/platform/asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"/opt/homebrew/Cellar/python@3.9/3.9.10/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\", line 596, in run_forever\n      self._run_once()\n    File \"/opt/homebrew/Cellar/python@3.9/3.9.10/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\", line 1890, in _run_once\n      handle._run()\n    File \"/opt/homebrew/Cellar/python@3.9/3.9.10/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/Users/snowy/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelbase.py\", line 513, in dispatch_queue\n      await self.process_one()\n    File \"/Users/snowy/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelbase.py\", line 502, in process_one\n      await dispatch(*args)\n    File \"/Users/snowy/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelbase.py\", line 409, in dispatch_shell\n      await result\n    File \"/Users/snowy/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelbase.py\", line 729, in execute_request\n      reply_content = await reply_content\n    File \"/Users/snowy/Library/Python/3.9/lib/python/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n      res = shell.run_cell(\n    File \"/Users/snowy/Library/Python/3.9/lib/python/site-packages/ipykernel/zmqshell.py\", line 540, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/Users/snowy/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_cell\n      result = self._run_cell(\n    File \"/Users/snowy/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3061, in _run_cell\n      result = runner(coro)\n    File \"/Users/snowy/Library/Python/3.9/lib/python/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/Users/snowy/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3266, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/Users/snowy/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3445, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/Users/snowy/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/var/folders/yp/_xstfn5s2nb1lmynh4_4ghmc0000gn/T/ipykernel_31675/3604603000.py\", line 1, in <module>\n      history = model.fit(x_train, y_train, batch_size = batch_size , validation_data = (X_test,y_test) , epochs = epochs , callbacks = [learning_rate_reduction])\n    File \"/opt/homebrew/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/opt/homebrew/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1791, in fit\n      val_logs = self.evaluate(\n    File \"/opt/homebrew/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/opt/homebrew/lib/python3.9/site-packages/keras/src/engine/training.py\", line 2200, in evaluate\n      logs = test_function_runner.run_step(\n    File \"/opt/homebrew/lib/python3.9/site-packages/keras/src/engine/training.py\", line 4000, in run_step\n      tmp_logs = self._function(dataset_or_iterator)\n    File \"/opt/homebrew/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1972, in test_function\n      return step_function(self, iterator)\n    File \"/opt/homebrew/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1956, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/homebrew/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1944, in run_step\n      outputs = model.test_step(data)\n    File \"/opt/homebrew/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1850, in test_step\n      y_pred = self(x, training=False)\n    File \"/opt/homebrew/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/opt/homebrew/lib/python3.9/site-packages/keras/src/engine/training.py\", line 569, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/opt/homebrew/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/opt/homebrew/lib/python3.9/site-packages/keras/src/engine/base_layer.py\", line 1150, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/opt/homebrew/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/opt/homebrew/lib/python3.9/site-packages/keras/src/engine/sequential.py\", line 404, in call\n      return super().call(inputs, training=training, mask=mask)\n    File \"/opt/homebrew/lib/python3.9/site-packages/keras/src/engine/functional.py\", line 512, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"/opt/homebrew/lib/python3.9/site-packages/keras/src/engine/functional.py\", line 651, in _run_internal_graph\n      y = self._conform_to_reference_input(y, ref_input=x)\n    File \"/opt/homebrew/lib/python3.9/site-packages/keras/src/engine/functional.py\", line 748, in _conform_to_reference_input\n      tensor = tf.cast(tensor, dtype=ref_input.dtype)\nNode: 'sequential_1/Cast'\nCast string to float is not supported\n\t [[{{node sequential_1/Cast}}]] [Op:__inference_test_function_6205]"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, batch_size = batch_size , validation_data = (X_test,y_test) , epochs = epochs , callbacks = [learning_rate_reduction])\n",
    "\n",
    "print(\"Accuracy of the model on Training Data is - \" , model.evaluate(x_train,y_train)[1]*100 , \"%\")\n",
    "print(\"Accuracy of the model on Testing Data is - \" , model.evaluate(X_test,y_test)[1]*100 , \"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
